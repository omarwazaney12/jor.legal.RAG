##!/usr/bin/env python3
"""
Advanced Legal RAG System for Jordanian Laws
Handles 101+ legal documents with sophisticated retrieval and reasoning
"""

import os
import json
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import re
from datetime import datetime
import hashlib
import time
import pickle

# Core libraries
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import chromadb
from chromadb.config import Settings

# ML and NLP
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# For retry logic
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60, backoff_factor=2):
    """Decorator for retrying functions with exponential backoff"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()
                    
                    # Check for rate limiting
                    if "rate limit" in error_msg or "quota" in error_msg:
                        delay = min(base_delay * (backoff_factor ** attempt), max_delay)
                        print(f"   â³ Rate limit hit. Waiting {delay}s before retry {attempt + 1}/{max_retries}...")
                        time.sleep(delay)
                        continue
                    
                    # Check for network/connection errors
                    elif any(term in error_msg for term in ["connection", "timeout", "network", "unreachable"]):
                        delay = min(base_delay * (backoff_factor ** attempt), max_delay)
                        print(f"   ğŸŒ Network error. Retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")
                        time.sleep(delay)
                        continue
                    
                    # For other errors, don't retry immediately
                    else:
                        if attempt < max_retries:
                            print(f"   âš ï¸  Error: {e}. Retrying... (attempt {attempt + 1}/{max_retries})")
                            time.sleep(1)
                        else:
                            print(f"   âŒ Failed after {max_retries} retries: {e}")
                            break
            
            raise last_exception
        return wrapper
    return decorator

@dataclass
class LegalDocument:
    """Enhanced document representation"""
    id: str
    title: str
    source_file: str
    content: str
    doc_type: str  # "Ù‚Ø§Ù†ÙˆÙ†", "Ù†Ø¸Ø§Ù…", "ØªØ¹Ù„ÙŠÙ…Ø§Øª"
    category: str  # Legal domain
    law_number: Optional[str]
    year: Optional[str]
    articles: List[Dict[str, str]]  # Extracted articles
    metadata: Dict[str, Any]
    word_count: int
    language: str = "arabic"

@dataclass
class QueryResult:
    """Enhanced query result with confidence and reasoning"""
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float
    reasoning_steps: List[str]
    citations: List[str]
    query_type: str
    processing_time: float

class AdvancedLegalProcessor:
    """Advanced document processor with legal-specific intelligence"""
    
    def __init__(self):
        self.article_patterns = [
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*\(?(\d+)\)?',
            r'Ø§Ù„Ø¨Ù†Ø¯\s*\(?(\d+)\)?', 
            r'Ø§Ù„ÙÙ‚Ø±Ø©\s*\(?(\d+)\)?',
            r'Ø§Ù„Ø¨Ù€Ù€Ù€Ù€Ù€Ø§Ø¨\s*\(?(\d+)\)?'
        ]
        
        self.law_metadata_patterns = {
            'law_number': r'Ø±Ù‚Ù…\s*\(?(\d+)\)?\s*Ù„Ø³Ù†Ø©\s*(\d+)',
            'ministry': r'ÙˆØ²Ø§Ø±Ø©\s+([\u0600-\u06FF\s]+)',
            'effective_date': r'ÙŠØ¹Ù…Ù„\s+Ø¨Ù‡\s+.*?(\d{4})',
            'published_date': r'Ù†Ø´Ø±Ù‡?\s+ÙÙŠ\s+Ø§Ù„Ø¬Ø±ÙŠØ¯Ø©\s+Ø§Ù„Ø±Ø³Ù…ÙŠØ©'
        }
        
    def normalize_arabic_number(self, number: str) -> str:
        """Normalize Arabic and Arabic-Indic numerals to standard format"""
        # Arabic-Indic to Arabic numerals mapping
        arabic_indic_to_arabic = {
            'Ù ': '0', 'Ù¡': '1', 'Ù¢': '2', 'Ù£': '3', 'Ù¤': '4',
            'Ù¥': '5', 'Ù¦': '6', 'Ù§': '7', 'Ù¨': '8', 'Ù©': '9'
        }
        
        # Convert Arabic-Indic numerals to Arabic
        normalized = number
        for arabic_indic, arabic in arabic_indic_to_arabic.items():
            normalized = normalized.replace(arabic_indic, arabic)
        
        # Remove leading zeros and normalize
        normalized = re.sub(r'^0+', '', normalized)
        if not normalized or not normalized.isdigit():  # If all zeros or non-numeric
            normalized = '0'
        
        return normalized

    def extract_articles(self, content: str) -> List[Dict[str, str]]:
        """Extract individual articles from legal text with deduplication"""
        articles = []
        seen_articles = set()  # Track normalized numbers to avoid duplicates
        
        # Enhanced patterns to catch various article formats
        patterns = [
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*\(?([Ù -Ù©0-9]+)\)?\s*:?\s*',
            r'Ø§Ù„Ø¨Ù†Ø¯\s*\(?([Ù -Ù©0-9]+)\)?\s*:?\s*',
            r'Ø§Ù„ÙÙ‚Ø±Ø©\s*\(?([Ù -Ù©0-9]+)\)?\s*:?\s*'
        ]
        
        for pattern in patterns:
            matches = list(re.finditer(pattern, content, re.MULTILINE))
            
            for i, match in enumerate(matches):
                raw_number = match.group(1)
                normalized_number = self.normalize_arabic_number(raw_number)
                
                # Skip if we've already seen this normalized number
                if normalized_number in seen_articles:
                    continue
                    
                seen_articles.add(normalized_number)
                
                # Extract article content until next article
                start_pos = match.end()
                if i + 1 < len(matches):
                    end_pos = matches[i + 1].start()
                    article_content = content[start_pos:end_pos].strip()
                else:
                    article_content = content[start_pos:].strip()
                
                # Clean up content
                article_content = re.sub(r'\n+', ' ', article_content)
                article_content = article_content.strip()
                
                if len(article_content) > 10:  # Only add if has substantial content
                    article_title = article_content[:100] + '...' if len(article_content) > 100 else article_content
                    
                    articles.append({
                        'number': normalized_number,
                        'title': article_title,
                        'content': article_content,
                        'full_text': f"Ø§Ù„Ù…Ø§Ø¯Ø© {normalized_number}: {article_content}"
                    })
        
        # Sort by numeric value
        return sorted(articles, key=lambda x: int(x['number']) if x['number'].isdigit() else 999)
    
    def extract_legal_metadata(self, title: str, content: str) -> Dict[str, Any]:
        """Extract comprehensive legal metadata"""
        metadata = {
            'extracted_at': datetime.now().isoformat(),
            'title_cleaned': self.clean_title(title)
        }
        
        # Extract law number and year
        law_match = re.search(self.law_metadata_patterns['law_number'], content)
        if law_match:
            metadata['law_number'] = law_match.group(1)
            metadata['year'] = law_match.group(2)
        
        # Extract ministry
        ministry_match = re.search(self.law_metadata_patterns['ministry'], content)
        if ministry_match:
            metadata['ministry'] = ministry_match.group(1).strip()
        
        # Document statistics
        metadata['total_articles'] = len(re.findall(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*\d+', content))
        metadata['total_chapters'] = len(re.findall(r'Ø§Ù„Ø¨Ø§Ø¨\s*\d+', content))
        metadata['total_items'] = len(re.findall(r'Ø§Ù„Ø¨Ù†Ø¯\s*\d+', content))
        
        # Legal terms frequency
        legal_terms = ['Ø­Ù‚ÙˆÙ‚', 'ÙˆØ§Ø¬Ø¨Ø§Øª', 'Ø§Ù„ØªØ²Ø§Ù…Ø§Øª', 'Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª', 'Ø¹Ù‚ÙˆØ¨Ø§Øª', 
                      'Ù…Ø®Ø§Ù„ÙØ§Øª', 'Ø±Ø³ÙˆÙ…', 'Ø¶Ø±Ø§Ø¦Ø¨', 'ØªØ±Ø®ÙŠØµ', 'ØªØ³Ø¬ÙŠÙ„', 'Ù…ÙˆØ§ÙÙ‚Ø©']
        
        metadata['legal_terms_found'] = []
        for term in legal_terms:
            if term in content:
                metadata['legal_terms_found'].append(term)
        
        return metadata
    
    def clean_title(self, title: str) -> str:
        """Clean and standardize document titles"""
        # Remove file extensions and common prefixes
        cleaned = title.replace('.txt', '').replace('_', ' ').replace('-', ' ')
        cleaned = re.sub(r'^(laws_|systems_|instructions_)', '', cleaned)
        return cleaned.strip()
    
    def categorize_document(self, title: str, content: str) -> Tuple[str, str]:
        """Advanced document categorization"""
        title_lower = title.lower()
        content_lower = content.lower()
        
        # Document type classification
        if title.startswith('laws_') or 'Ù‚Ø§Ù†ÙˆÙ†' in title_lower:
            doc_type = "Ù‚Ø§Ù†ÙˆÙ†"
        elif title.startswith('systems_') or 'Ù†Ø¸Ø§Ù…' in title_lower:
            doc_type = "Ù†Ø¸Ø§Ù…"
        elif title.startswith('instructions_') or 'ØªØ¹Ù„ÙŠÙ…Ø§Øª' in title_lower:
            doc_type = "ØªØ¹Ù„ÙŠÙ…Ø§Øª"
        elif 'Ø§Ù„Ø¯Ø³ØªÙˆØ±' in title_lower:
            doc_type = "Ø¯Ø³ØªÙˆØ±"
        else:
            doc_type = "Ù…ØªÙ†ÙˆØ¹Ø©"
        
        # Legal domain classification
        domain_keywords = {
            'Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ': ['Ù…Ø³ØªÙ‡Ù„Ùƒ', 'Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ', 'Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ†'],
            'Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø´Ø±ÙƒØ§Øª': ['Ø´Ø±ÙƒØ©', 'Ø´Ø±ÙƒØ§Øª', 'Ù…Ø³Ø§Ù‡Ù…Ø©', 'Ù…Ø­Ø¯ÙˆØ¯Ø©'],
            'ØªØ¬Ø§Ø±Ø© Ø®Ø§Ø±Ø¬ÙŠØ©': ['Ø§Ø³ØªÙŠØ±Ø§Ø¯', 'ØªØµØ¯ÙŠØ±', 'Ø¬Ù…Ø§Ø±Ùƒ', 'ØªØ¹Ø±ÙØ©'],
            'ØªØ³Ø¬ÙŠÙ„ ØªØ¬Ø§Ø±ÙŠ': ['ØªØ³Ø¬ÙŠÙ„ ØªØ¬Ø§Ø±ÙŠ', 'Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ'],
            'Ø§Ø³ØªØ«Ù…Ø§Ø±': ['Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©'],
            'Ø¹Ù…Ù„ ÙˆØ¶Ù…Ø§Ù†': ['Ø¹Ù…Ù„', 'Ø¹Ù…Ø§Ù„', 'Ø¶Ù…Ø§Ù† Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ'],
            'Ø¨ÙŠØ¦Ø©': ['Ø¨ÙŠØ¦Ø©', 'Ø¨ÙŠØ¦ÙŠ', 'ØªÙ„ÙˆØ«'],
            'Ù…Ø§Ù„ÙŠØ© ÙˆØ¶Ø±Ø§Ø¦Ø¨': ['Ø¶Ø±ÙŠØ¨Ø©', 'Ø¶Ø±Ø§Ø¦Ø¨', 'Ø±Ø³ÙˆÙ…', 'Ù…Ø§Ù„ÙŠØ©'],
            'Ù…Ù„ÙƒÙŠØ© ÙÙƒØ±ÙŠØ©': ['Ø¨Ø±Ø§Ø¡Ø©', 'Ø¹Ù„Ø§Ù…Ø© ØªØ¬Ø§Ø±ÙŠØ©', 'Ù…Ù„ÙƒÙŠØ© ÙÙƒØ±ÙŠØ©']
        }
        
        category = "Ø¹Ø§Ù…"  # Default
        max_matches = 0
        
        for domain, keywords in domain_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in content_lower or keyword in title_lower)
            if matches > max_matches:
                max_matches = matches
                category = domain
        
        return doc_type, category

class AdvancedVectorStore:
    """Advanced vector storage with hybrid search capabilities"""
    
    def __init__(self, collection_name: str = "jordanian_legal_docs"):
        self.collection_name = collection_name
        
        # Initialize embeddings as None - will be created when needed for queries
        self.embeddings = None
        
        # Initialize ChromaDB with in-memory client (no SQLite issues)
        self.chroma_client = chromadb.Client()  # In-memory client
        
        # Progress tracking (for backward compatibility)
        self.progress_file = Path("embedding_progress.pkl")
        self.error_log_file = Path("embedding_errors.log")
        
        # Initialize collection as None - will be loaded from pre-built data
        self.collection = None
        
        # TF-IDF for keyword search
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # Keep Arabic stopwords for legal precision
            ngram_range=(1, 3)
        )
        self.tfidf_matrix = None
        self.doc_chunks = []
    
    def clean_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Clean metadata by removing None values and ensuring ChromaDB compatibility"""
        cleaned = {}
        for key, value in metadata.items():
            if value is not None:
                # Convert all values to strings, ints, floats, or bools for ChromaDB compatibility
                if isinstance(value, (str, int, float, bool)):
                    cleaned[key] = value
                else:
                    cleaned[key] = str(value)
        return cleaned
    
    def save_progress(self, completed_chunks: int, total_chunks: int, embeddings: List, chunk_metadata: List):
        """Save progress to resume later if needed"""
        progress_data = {
            'completed_chunks': completed_chunks,
            'total_chunks': total_chunks,
            'embeddings': embeddings,
            'chunk_metadata': chunk_metadata,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(self.progress_file, 'wb') as f:
            pickle.dump(progress_data, f)
        print(f"   ğŸ’¾ Progress saved: {completed_chunks}/{total_chunks} chunks processed")
    
    def load_progress(self) -> Optional[Dict]:
        """Load previous progress if available"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"   âš ï¸  Could not load progress: {e}")
                return None
        return None
    
    def log_error(self, error: str, chunk_index: int = None):
        """Log errors to file for debugging"""
        timestamp = datetime.now().isoformat()
        error_entry = f"[{timestamp}] Chunk {chunk_index}: {error}\n"
        
        with open(self.error_log_file, 'a', encoding='utf-8') as f:
            f.write(error_entry)
    
    @retry_with_backoff(max_retries=5, base_delay=2, max_delay=120)
    def generate_embeddings_batch(self, chunks: List[str]) -> List[List[float]]:
        """Generate embeddings with retry logic"""
        return self.embeddings.embed_documents(chunks)
    
    def clear_collection(self):
        """Clear the existing collection to rebuild with clean metadata"""
        try:
            self.chroma_client.delete_collection(self.collection_name)
            print(f"ğŸ—‘ï¸  Cleared existing collection: {self.collection_name}")
        except Exception as e:
            print(f"â„¹ï¸  No existing collection to clear: {e}")
        
        # Recreate collection
        self.collection = self.chroma_client.create_collection(
            name=self.collection_name,
            metadata={"description": "Jordanian Legal Documents"}
        )
        print(f"âœ… Created fresh collection: {self.collection_name}")

    def load_prebuilt_embeddings(self):
        """Load pre-built embeddings from JSON file"""
        print("ğŸ“– Loading pre-built embeddings...")
        
        # Try multiple possible paths for the embeddings file
        possible_paths = [
            Path("railway_embeddings_data/embeddings_data.json"),
            Path("./railway_embeddings_data/embeddings_data.json"),
            Path("/app/railway_embeddings_data/embeddings_data.json"),
            Path("embeddings_data.json")
        ]
        
        embeddings_file = None
        for path in possible_paths:
            print(f"ğŸ” Checking path: {path}")
            if path.exists():
                embeddings_file = path
                print(f"âœ… Found embeddings at: {path}")
                break
        
        if not embeddings_file:
            print("âŒ Pre-built embeddings not found in any of these locations:")
            for path in possible_paths:
                print(f"   - {path}")
            print("ğŸ“ Current working directory contents:")
            import os
            print(f"   Working dir: {os.getcwd()}")
            for item in os.listdir("."):
                print(f"   - {item}")
            return False
        
        try:
            with open(embeddings_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“Š Loaded {len(data['documents'])} pre-built documents")
            
            # Create collection
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "Jordanian legal documents for RAG system",
                    "model": data.get('model', 'text-embedding-ada-002'),
                    "total_docs": data.get('total_docs', len(data['documents']))
                }
            )
            
            # Add all data to ChromaDB
            print("ğŸ”„ Adding documents to ChromaDB...")
            self.collection.add(
                documents=data['documents'],
                metadatas=data['metadatas'],
                embeddings=data['embeddings'],
                ids=data['ids']
            )
            
            # Build TF-IDF index for hybrid search
            print("ğŸ” Building TF-IDF index for hybrid search...")
            self.doc_chunks = data['documents']
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.doc_chunks)
            
            final_count = self.collection.count()
            print(f"âœ… Successfully loaded {final_count} documents into ChromaDB")
            print(f"âœ… TF-IDF index ready with {len(self.doc_chunks)} chunks")
            
            return True
                
        except Exception as e:
            print(f"âŒ Error loading pre-built embeddings: {e}")
            return False

    def add_documents(self, documents: List[LegalDocument], chunk_size: int = 500, force_rebuild: bool = False):
        """Load documents using pre-built embeddings"""
        print(f"ğŸ“„ Using pre-built embeddings for {len(documents)} documents...")
        
        # Try to load pre-built embeddings first
        if self.load_prebuilt_embeddings():
            return
        
        # Fallback: If pre-built embeddings not available, show error
        print("âŒ Pre-built embeddings not available. Please ensure railway_embeddings_data/embeddings_data.json exists.")
        print("ğŸ”§ Run build_raw_embeddings.py to generate embeddings first.")
    
    def _get_embeddings(self):
        """Lazy initialization of embeddings only when needed"""
        if self.embeddings is None:
            # Use completely isolated OpenAI client to avoid Railway interference
            import os
            import json
            import requests
            
            # Store HTTP client for direct API calls bypassing openai module
            self.embeddings = {
                'api_key': os.getenv('OPENAI_API_KEY'),
                'base_url': 'https://api.openai.com/v1/embeddings'
            }
        return self.embeddings
    
    def hybrid_search(self, query: str, top_k: int = 20, semantic_weight: float = 0.7) -> List[Dict]:
        """Advanced hybrid search combining semantic and keyword matching"""
        
        # 1. Semantic search using embeddings (direct HTTP API to avoid Railway issues)
        openai_config = self._get_embeddings()
        try:
            import requests
            import json
            
            headers = {
                'Authorization': f"Bearer {openai_config['api_key']}",
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'text-embedding-ada-002',
                'input': query
            }
            
            response = requests.post(
                openai_config['base_url'],
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                query_embedding = result['data'][0]['embedding']
            else:
                print(f"âš ï¸ OpenAI API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"âš ï¸ Embedding generation failed: {e}")
            # Fallback: return empty results if embeddings fail
            return []
        
        semantic_results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k * 2,  # Get more for reranking
            include=['documents', 'metadatas', 'distances']
        )
        
        # 2. Keyword search using TF-IDF
        keyword_scores = []
        if self.tfidf_matrix is not None and len(self.doc_chunks) > 0:
            query_tfidf = self.tfidf_vectorizer.transform([query])
            keyword_similarities = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()
            keyword_scores = keyword_similarities.tolist()  # Convert to list to avoid numpy array issues
        
        # 3. Combine scores and rerank
        combined_results = []
        
        for i, (doc, metadata, distance) in enumerate(zip(
            semantic_results['documents'][0],
            semantic_results['metadatas'][0], 
            semantic_results['distances'][0]
        )):
            # Convert distance to similarity (lower distance = higher similarity)
            semantic_score = max(0, 1 - distance)
            
            # Get keyword score for this document
            keyword_score = 0
            if keyword_scores and i < len(keyword_scores):
                keyword_score = float(keyword_scores[i])  # Ensure it's a float
            
            # Combined score
            final_score = (semantic_weight * semantic_score + 
                          (1 - semantic_weight) * keyword_score)
            
            combined_results.append({
                'content': doc,
                'metadata': metadata,
                'semantic_score': semantic_score,
                'keyword_score': keyword_score,
                'final_score': final_score
            })
        
        # Sort by final score and return top_k
        combined_results.sort(key=lambda x: x['final_score'], reverse=True)
        return combined_results[:top_k]

class QueryClassifier:
    """Classify legal queries to optimize retrieval strategy"""
    
    def __init__(self):
        self.query_types = {
            'procedure': ['Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª', 'Ø®Ø·ÙˆØ§Øª', 'ÙƒÙŠÙÙŠØ©', 'Ø·Ø±ÙŠÙ‚Ø©', 'Ù…Ø±Ø§Ø­Ù„'],
            'requirements': ['Ø´Ø±ÙˆØ·', 'Ù…ØªØ·Ù„Ø¨Ø§Øª', 'Ø¶ÙˆØ§Ø¨Ø·', 'Ù…Ø¹Ø§ÙŠÙŠØ±'],
            'rights': ['Ø­Ù‚ÙˆÙ‚', 'Ø­Ù‚', 'Ù…ÙƒÙÙˆÙ„Ø©', 'Ù…Ø¶Ù…ÙˆÙ†Ø©'],
            'obligations': ['ÙˆØ§Ø¬Ø¨Ø§Øª', 'Ø§Ù„ØªØ²Ø§Ù…Ø§Øª', 'Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª'],
            'penalties': ['Ø¹Ù‚ÙˆØ¨Ø§Øª', 'ØºØ±Ø§Ù…Ø©', 'Ù…Ø®Ø§Ù„ÙØ©', 'Ø¬Ø²Ø§Ø¡'],
            'definitions': ['ØªØ¹Ø±ÙŠÙ', 'Ù…Ø¹Ù†Ù‰', 'Ø§Ù„Ù…Ù‚ØµÙˆØ¯', 'ÙŠÙ‚ØµØ¯'],
            'establishment': ['Ø¥Ù†Ø´Ø§Ø¡', 'ØªØ£Ø³ÙŠØ³', 'ÙØªØ­', 'Ø¥Ù‚Ø§Ù…Ø©'],
            'registration': ['ØªØ³Ø¬ÙŠÙ„', 'Ù‚ÙŠØ¯', 'ØªØµØ±ÙŠØ­', 'ØªØ±Ø®ÙŠØµ']
        }
    
    def classify(self, query: str) -> str:
        """Classify query type for optimized retrieval"""
        query_lower = query.lower()
        
        type_scores = {}
        for query_type, keywords in self.query_types.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                type_scores[query_type] = score
        
        if type_scores:
            return max(type_scores, key=type_scores.get)
        
        return 'general'

# Initialize the system
print("ğŸš€ Advanced Legal RAG System Loading...")

class LegalReasoningEngine:
    """Advanced reasoning engine for complex legal queries"""
    
    def __init__(self, vector_store: AdvancedVectorStore):
        self.vector_store = vector_store
        self.query_classifier = QueryClassifier()
        
        # Reasoning templates for different query types
        self.reasoning_templates = {
            'procedure': self._procedure_reasoning,
            'requirements': self._requirements_reasoning,
            'establishment': self._establishment_reasoning,
            'registration': self._registration_reasoning,
            'general': self._general_reasoning
        }
    
    def detect_language(self, text: str) -> str:
        """Detect if the text is primarily in Arabic or English"""
        import re
        
        # Count Arabic characters
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        
        # Count English/Latin characters
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        # Total meaningful characters
        total_meaningful = arabic_chars + english_chars
        
        if total_meaningful == 0:
            return 'arabic'  # Default to Arabic
        
        # If more than 60% Arabic characters, consider it Arabic
        if arabic_chars / total_meaningful > 0.6:
            return 'arabic'
        elif english_chars / total_meaningful > 0.6:
            return 'english'
        else:
            # Mixed or unclear, use some common keywords
            english_keywords = ['what', 'how', 'when', 'where', 'why', 'company', 'law', 'procedure', 'requirements']
            arabic_keywords = ['Ù…Ø§', 'ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'Ù„Ù…Ø§Ø°Ø§', 'Ø´Ø±ÙƒØ©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª', 'Ø´Ø±ÙˆØ·']
            
            text_lower = text.lower()
            english_found = sum(1 for keyword in english_keywords if keyword in text_lower)
            arabic_found = sum(1 for keyword in arabic_keywords if keyword in text_lower)
            
            return 'english' if english_found > arabic_found else 'arabic'
    
    def clean_response_formatting(self, response: str) -> str:
        """Clean and standardize response formatting"""
        import re
        
        # Remove all markdown formatting
        cleaned = response
        
        # Remove bold formatting (**text**)
        cleaned = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned)
        
        # Remove italic formatting (*text*)
        cleaned = re.sub(r'\*(.*?)\*', r'\1', cleaned)
        
        # Remove markdown headers (## text)
        cleaned = re.sub(r'^#{1,6}\s+', '', cleaned, flags=re.MULTILINE)
        
        # Standardize numbered list formatting
        # Pattern: "1. **Title:**" -> "1. Title:"
        cleaned = re.sub(r'^(\d+)\.\s*\*\*(.*?)\*\*:?\s*$', r'\1. \2:', cleaned, flags=re.MULTILINE)
        
        # Pattern: "1. Title:" (ensure colon at end)
        cleaned = re.sub(r'^(\d+)\.\s+([^:\n]+)(?::?)$', r'\1. \2:', cleaned, flags=re.MULTILINE)
        
        # Ensure bullet points use consistent dash formatting
        # Convert various bullet formats to standard dash
        cleaned = re.sub(r'^[\s]*[â€¢Â·â€§âƒâ–ªâ–«â€£âŒâ]*\s*', '- ', cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r'^[\s]*[\*\+]\s+', '- ', cleaned, flags=re.MULTILINE)
        
        # Fix multiple dashes to single dash
        cleaned = re.sub(r'^-+\s*', '- ', cleaned, flags=re.MULTILINE)
        
        # Ensure proper spacing after numbers
        cleaned = re.sub(r'^(\d+)\.([^\s])', r'\1. \2', cleaned, flags=re.MULTILINE)
        
        # Clean up excessive whitespace
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)
        
        # Enhanced line spacing formatting
        lines = cleaned.split('\n')
        formatted_lines = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                formatted_lines.append('')
                continue
            
            # Check if this is a numbered item (main title)
            is_numbered = re.match(r'^\d+\.\s+', line)
            # Check if this is a bullet point
            is_bullet = re.match(r'^-\s+', line)
            # Check if this is a reference line
            is_reference = 'Ø§Ù„Ù…Ø±Ø¬Ø¹' in line or 'Ø§Ù‚ØªØ¨Ø§Ø³' in line or 'Legal Reference' in line or 'Quote:' in line
            
            # Add extra spacing before numbered items (except first one)
            if is_numbered and formatted_lines and formatted_lines[-1] != '':
                formatted_lines.append('')
                formatted_lines.append('')
            
            # Add spacing before bullet points that aren't immediately after numbered items
            elif is_bullet and formatted_lines:
                prev_line = formatted_lines[-1] if formatted_lines else ''
                if prev_line and not re.match(r'^\d+\.\s+', prev_line) and not prev_line == '':
                    formatted_lines.append('')
            
            # Add spacing before references
            elif is_reference and formatted_lines and formatted_lines[-1] != '':
                if not re.match(r'^-\s+', formatted_lines[-1]):
                    formatted_lines.append('')
            
            formatted_lines.append(line)
            
            # Add spacing after numbered titles (before bullet points)
            if is_numbered:
                formatted_lines.append('')
        
        # Join lines and clean up
        cleaned = '\n'.join(formatted_lines)
        
        # Remove trailing spaces and clean up final formatting
        cleaned = '\n'.join(line.rstrip() for line in cleaned.split('\n'))
        
        # Final cleanup of excessive spacing
        cleaned = re.sub(r'\n{4,}', '\n\n\n', cleaned)
        
        # Ensure single newline at end
        cleaned = cleaned.strip() + '\n' if cleaned.strip() else ''
        
        return cleaned.strip()
    
    def process_query(self, query: str, conversation_history: List[str] = None) -> QueryResult:
        """Process complex legal query with multi-step reasoning"""
        start_time = datetime.now()
        
        # Step 1: Classify query type
        query_type = self.query_classifier.classify(query)
        print(f"ğŸ” Query classified as: {query_type}")
        
        # Step 2: Enhanced retrieval based on query type
        relevant_docs = self._enhanced_retrieval(query, query_type)
        
        # Step 3: Apply reasoning template
        reasoning_func = self.reasoning_templates.get(query_type, self._general_reasoning)
        result = reasoning_func(query, relevant_docs, conversation_history)
        
        # Step 4: Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        result.processing_time = processing_time
        result.query_type = query_type
        
        return result
    
    def _enhanced_retrieval(self, query: str, query_type: str, top_k: int = 10) -> List[Dict]:
        """Enhanced retrieval strategy based on query type"""
        
        # Adjust search parameters based on query type
        if query_type in ['procedure', 'requirements', 'establishment']:
            # For procedural queries, prefer recent regulations and instructions
            semantic_weight = 0.8  # Higher semantic matching
            top_k = 12  # Get more documents for comprehensive procedures
        elif query_type in ['definitions', 'rights']:
            # For definitions, prefer exact legal text matches
            semantic_weight = 0.5  # Balance semantic and keyword
            top_k = 6  # Fewer, more precise results
        else:
            semantic_weight = 0.7  # Default balance
        
        # Perform hybrid search
        results = self.vector_store.hybrid_search(
            query=query,
            top_k=top_k,
            semantic_weight=semantic_weight
        )
        
        # Filter and rerank based on query type
        if query_type == 'establishment':
            # Prioritize laws and systems over instructions for establishment queries
            results = self._rerank_for_establishment(results)
        elif query_type == 'procedure':
            # Prioritize instructions and systems for procedural queries
            results = self._rerank_for_procedures(results)
        
        return results
    
    def _rerank_for_establishment(self, results: List[Dict]) -> List[Dict]:
        """Rerank results for establishment queries"""
        for result in results:
            metadata = result['metadata']
            
            # Boost laws and systems
            if metadata.get('doc_type') == 'Ù‚Ø§Ù†ÙˆÙ†':
                result['final_score'] *= 1.3
            elif metadata.get('doc_type') == 'Ù†Ø¸Ø§Ù…':
                result['final_score'] *= 1.2
            
            # Boost relevant categories
            relevant_categories = ['Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø´Ø±ÙƒØ§Øª', 'ØªØ³Ø¬ÙŠÙ„ ØªØ¬Ø§Ø±ÙŠ']
            if metadata.get('category') in relevant_categories:
                result['final_score'] *= 1.2
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    
    def _rerank_for_procedures(self, results: List[Dict]) -> List[Dict]:
        """Rerank results for procedural queries"""
        for result in results:
            metadata = result['metadata']
            
            # Boost instructions and systems for procedures
            if metadata.get('doc_type') == 'ØªØ¹Ù„ÙŠÙ…Ø§Øª':
                result['final_score'] *= 1.3
            elif metadata.get('doc_type') == 'Ù†Ø¸Ø§Ù…':
                result['final_score'] *= 1.2
            
            # Boost article-level chunks for precise procedures
            if metadata.get('chunk_type') == 'article':
                result['final_score'] *= 1.1
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    
    def _procedure_reasoning(self, query: str, docs: List[Dict], history: List[str]) -> QueryResult:
        """Reasoning for procedural queries"""
        reasoning_steps = [
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©",
            "ØªØ±ØªÙŠØ¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ",
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ© Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù„Ø§Ø²Ù…Ø©"
        ]
        
        return self._generate_structured_answer(query, docs, reasoning_steps, "procedural", history)
    
    def _requirements_reasoning(self, query: str, docs: List[Dict], history: List[str]) -> QueryResult:
        """Reasoning for requirements queries"""
        reasoning_steps = [
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ù†Ø¸Ù…Ø©",
            "ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ©",
            "ØªØ±ØªÙŠØ¨ Ø§Ù„Ø´Ø±ÙˆØ· Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"
        ]
        
        return self._generate_structured_answer(query, docs, reasoning_steps, "requirements", history)
    
    def _establishment_reasoning(self, query: str, docs: List[Dict], history: List[str]) -> QueryResult:
        """Specialized reasoning for business establishment queries"""
        reasoning_steps = [
            "ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø´Ø£Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ£Ø³ÙŠØ³Ù‡Ø§",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù„Ù„ØªØ£Ø³ÙŠØ³",
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©", 
            "ØªØ±ØªÙŠØ¨ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ£Ø³ÙŠØ³ Ø²Ù…Ù†ÙŠØ§Ù‹",
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Øª ÙˆØ§Ù„ÙˆØ²Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ©"
        ]
        
        return self._generate_structured_answer(query, docs, reasoning_steps, "establishment", history)
    
    def _registration_reasoning(self, query: str, docs: List[Dict], history: List[str]) -> QueryResult:
        """Reasoning for registration queries"""
        reasoning_steps = [
            "ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø±ÙˆØ· Ø§Ù„ØªØ³Ø¬ÙŠÙ„",
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©",
            "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… ÙˆØ§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ©"
        ]
        
        return self._generate_structured_answer(query, docs, reasoning_steps, "registration", history)
    
    def _general_reasoning(self, query: str, docs: List[Dict], history: List[str]) -> QueryResult:
        """General reasoning for other queries"""
        reasoning_steps = [
            "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ",
            "Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©",
            "ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙÙ‚ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"
        ]
        
        return self._generate_structured_answer(query, docs, reasoning_steps, "general", history)
    
    def _generate_structured_answer(self, query: str, docs: List[Dict], reasoning_steps: List[str], answer_type: str, conversation_history: List[str] = None) -> QueryResult:
        """Generate structured answer using advanced prompting"""
        
        # Better document filtering for relevance
        filtered_docs = []
        query_keywords = set(query.lower().split())
        
        for doc in docs:
            doc_text = doc.get('content', '').lower()
            doc_title = doc.get('title', '').lower()
            
            # Calculate relevance score based on keyword overlap and content quality
            title_matches = sum(1 for keyword in query_keywords if keyword in doc_title)
            content_matches = sum(1 for keyword in query_keywords if keyword in doc_text)
            
            # Prioritize documents with title matches and substantial content
            relevance_score = (title_matches * 3) + content_matches
            doc_length = len(doc_text)
            
            # Filter out clearly irrelevant documents (like factory regulations for business registration)
            irrelevant_patterns = ['Ù…ØµØ§Ù†Ø¹', 'Ù…Ø®Ø§Ø¨Ø²', 'Ù…Ø¹Ø§Ø±Ø¶'] if any(word in query.lower() for word in ['Ø´Ø±ÙƒØ©', 'ØªØ³Ø¬ÙŠÙ„', 'business', 'registration']) else []
            
            is_irrelevant = any(pattern in doc_text for pattern in irrelevant_patterns)
            
            if relevance_score > 0 and doc_length > 100 and not is_irrelevant:
                filtered_docs.append({**doc, 'relevance_score': relevance_score})
        
        # Sort by relevance and take top documents
        filtered_docs.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        top_docs = filtered_docs[:8]  # Focus on most relevant documents
        
        # Build context from filtered documents
        context = ""
        sources = []
        citations = []
        
        for i, doc in enumerate(top_docs):
            content = doc.get('content', '')
            title = doc.get('title', f'Document {i+1}')
            
            # Truncate very long documents but keep key information
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            context += f"\n=== {title} ===\n{content}\n"
            
            sources.append({
                'title': title,
                'confidence': min(0.95, 0.7 + (doc.get('relevance_score', 0) * 0.05)),
                'snippet': content[:300] + "..." if len(content) > 300 else content
            })
            
            citations.append(f"Ø§Ù„Ù…Ø±Ø¬Ø¹: {title}")
        
        # Conversation context
        conversation_context = ""
        if conversation_history:
            conversation_context = f"\n\nContext from previous conversation:\n{' '.join(conversation_history[-3:])}\n"
        
        # Get conversation context if available
        detected_language = self.detect_language(query)
        
        # Enhanced prompts with better instructions
        if detected_language == 'english':
            if answer_type == "establishment":
                prompt = f"""You are a legal expert specialized in Jordanian laws. Answer the following question based on the legal documents provided.

Jordanian Legal Documents:
{context}{conversation_context}

Question: {query}

CRITICAL INSTRUCTIONS FOR HIGH-QUALITY RESPONSES:
1. Start with a comprehensive title summarizing the topic followed by a colon
2. Organize the answer in detailed numbered points, each point containing:
   - A detailed descriptive title followed by a colon (without any formatting symbols)
   - Multiple comprehensive sub-points starting with dash (-) covering all aspects
   - Specific legal reference for each sub-point using actual law names and articles
   - Additional details and practical information for each point
3. Use this EXACT format for references: "Legal Reference: [Exact Law name and number for year], Article [article number]"
4. Do not use any formatting symbols like ** or * or ##
5. Use extensive direct quotes from the legal texts provided above
6. Specify in comprehensive detail: competent government agencies, exact timeframes, precise fees in JOD, required documents, conditions
7. Arrange steps chronologically for implementation with detailed explanation of each step
8. Include all exceptions, special cases, and additional regulations
9. ONLY use information from the legal documents provided above - do not mention document limitations
10. Use specific law names and article numbers from the provided documents
11. Provide comprehensive coverage of all aspects related to the topic
12. Include supplementary information and practical guidance from the documents
13. Focus on actionable information and specific requirements
14. End with: "Notice: This information is for general guidance only. Consult a qualified attorney for personal legal advice."

Answer:"""
            
            elif answer_type == "procedural":
                prompt = f"""You are a legal expert specialized in Jordanian laws. Explain the complete legal procedures based on the provided documents.

Jordanian Legal Documents:
{context}{conversation_context}

Question: {query}

CRITICAL INSTRUCTIONS FOR COMPREHENSIVE PROCEDURAL GUIDANCE:
1. Start with a title summarizing all procedures followed by a colon
2. Organize the answer in detailed numbered points, each point containing:
   - A descriptive title for each procedure followed by a colon (without any formatting symbols)
   - Multiple detailed sub-points starting with dash (-) explaining procedure specifics
   - Specific legal reference for each sub-point with exact law citations
   - Step-by-step breakdown of each major procedure
3. Use this format for references: "Legal Reference: [Exact Law name and number for year], Article [article number]"
4. Do not use any formatting symbols like ** or * or ##
5. Use extensive direct quotes from the legal texts provided above
6. Specify in detail: competent agency, all required documents, exact timeframes, precise costs
7. Clarify all special conditions, requirements, and regulatory controls
8. Provide complete procedural workflow from start to finish
9. ONLY use information from the provided legal documents
10. Focus on practical implementation steps and requirements
11. End with: "Notice: This information is for general guidance only. Consult a qualified attorney for personal legal advice."

Required Procedures:"""
            
            else:  # general, requirements, registration
                prompt = f"""You are a legal expert specialized in Jordanian laws. Provide a comprehensive answer based on the legal documents provided.

Jordanian Legal Documents:
{context}{conversation_context}

Question: {query}

CRITICAL INSTRUCTIONS FOR COMPREHENSIVE LEGAL GUIDANCE:
1. Start with a descriptive title summarizing the topic followed by a colon
2. Organize the answer in detailed numbered points, each point containing:
   - A comprehensive descriptive title followed by a colon (without any formatting symbols)
   - Multiple detailed sub-points starting with dash (-) explaining all aspects
   - Specific legal reference for each sub-point with exact citations
   - Practical details and implementation guidance
3. Use this format for references: "Legal Reference: [Exact Law name and number for year], Article [article number]"
4. Do not use any formatting symbols like ** or * or ##
5. Use direct quotes from the legal texts provided above in format: "Quote: [literal text]"
6. Clarify all exceptions, special conditions, and different scenarios
7. Provide comprehensive technical, procedural, and practical details
8. ONLY use information found in the provided legal documents
9. Focus on specific, actionable information rather than general statements
10. Include all relevant costs, timeframes, and requirements in detail
11. End with: "Notice: This information is for general guidance only. Consult a qualified attorney for personal legal advice."

Answer:"""
        
        else:  # Arabic prompts (default)
            if answer_type == "establishment":
                prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©. Ù‚Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø©.

Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©:
{context}{conversation_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­Ø§Ø³Ù…Ø© Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© (Ø£Ø¬Ø¨ Ø¨Ø£ÙƒØ¨Ø± Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø´Ù…ÙˆÙ„ÙŠØ©):
1. Ø§Ø¨Ø¯Ø£ Ø¨Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ù…Ù„ ÙŠÙ„Ø®Øµ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…ØªØ¨ÙˆØ¹Ø§Ù‹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ†
2. Ù†Ø¸Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ØµÙŠØºØ© Ù†Ù‚Ø§Ø· Ù…Ø±Ù‚Ù…Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙØµÙ„Ø©ØŒ ÙƒÙ„ Ù†Ù‚Ø·Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
   - Ø¹Ù†ÙˆØ§Ù† ÙˆØµÙÙŠ Ù…ÙØµÙ„ Ù…ØªØ¨ÙˆØ¹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ† (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚)
   - Ù†Ù‚Ø§Ø· ÙØ±Ø¹ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆØ´Ø§Ù…Ù„Ø© ØªØ¨Ø¯Ø£ Ø¨Ø´Ø±Ø·Ø© (-) ØªØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨
   - Ù…Ø±Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø­Ø¯Ø¯ Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© ÙØ±Ø¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„ØµØ­ÙŠØ­Ø©
   - ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© ÙˆØ¥Ø±Ø´Ø§Ø¯Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø©
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹: "Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: [Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ±Ù‚Ù…Ù‡ ÙˆÙ„Ø³Ù†ØªÙ‡]ØŒ Ø§Ù„Ù…Ø§Ø¯Ø© [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©]"
4. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚ Ù…Ø«Ù„ ** Ø£Ùˆ * Ø£Ùˆ ##
5. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
6. Ø­Ø¯Ø¯ Ø¨ØªÙØµÙŠÙ„ Ø´Ø§Ù…Ù„: Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ø§Ù„Ù…Ø®ØªØµØ©ØŒ Ø§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø§Ù„Ø±Ø³ÙˆÙ… Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØŒ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ÙˆØ§Ø¬Ø¨ ØªÙˆÙØ±Ù‡Ø§
7. Ø±ØªØ¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„ØªÙ†ÙÙŠØ° Ù…Ø¹ Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©
8. Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª ÙˆØ§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„Ø¶ÙˆØ§Ø¨Ø· Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
9. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
10. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
11. Ù‚Ø¯Ù… Ø´Ø±Ø­Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹ ÙŠØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
12. Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ ÙˆØ§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„
13. Ø§Ø®ØªØªÙ… Ø¨Ù€: "ØªÙ†Ø¨ÙŠÙ‡: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·. Ø§Ø³ØªØ´Ø± Ù…Ø­Ø§Ù…ÙŠØ§Ù‹ Ù…Ø¤Ù‡Ù„Ø§Ù‹ Ù„Ù„Ù…Ø´ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø´Ø®ØµÙŠØ©."

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
            
            elif answer_type == "procedural":
                prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©. Ù‚Ù… Ø¨Ø´Ø±Ø­ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±ÙÙ‚Ø©.

Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©:
{context}{conversation_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­Ø§Ø³Ù…Ø© Ù„Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ (Ù‚Ø¯Ù… Ø£Ø´Ù…Ù„ ÙˆØ£Ø¯Ù‚ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©):
1. Ø§Ø¨Ø¯Ø£ Ø¨Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ù…Ù„ ÙŠÙ„Ø®Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…ØªØ¨ÙˆØ¹Ø§Ù‹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ†
2. Ù†Ø¸Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ØµÙŠØºØ© Ù†Ù‚Ø§Ø· Ù…Ø±Ù‚Ù…Ø© Ù…ÙØµÙ„Ø© ÙˆØ´Ø§Ù…Ù„Ø©ØŒ ÙƒÙ„ Ù†Ù‚Ø·Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
   - Ø¹Ù†ÙˆØ§Ù† ÙˆØµÙÙŠ Ù…ÙØµÙ„ Ù„ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡ Ù…ØªØ¨ÙˆØ¹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ† (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚)
   - Ù†Ù‚Ø§Ø· ÙØ±Ø¹ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ…ÙØµÙ„Ø© ØªØ¨Ø¯Ø£ Ø¨Ø´Ø±Ø·Ø© (-) ØªØ´Ø±Ø­ Ø¬Ù…ÙŠØ¹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
   - Ù…Ø±Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø­Ø¯Ø¯ Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© ÙØ±Ø¹ÙŠØ© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„ØµØ­ÙŠØ­Ø©
   - Ø®Ø·ÙˆØ§Øª ÙØ±Ø¹ÙŠØ© ØªØ­Øª ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡ Ø±Ø¦ÙŠØ³ÙŠ
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹: "Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: [Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ±Ù‚Ù…Ù‡ ÙˆÙ„Ø³Ù†ØªÙ‡]ØŒ Ø§Ù„Ù…Ø§Ø¯Ø© [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©]"
4. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚ Ù…Ø«Ù„ ** Ø£Ùˆ * Ø£Ùˆ ##
5. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
6. Ø­Ø¯Ø¯ Ø¨ØªÙØµÙŠÙ„ ÙƒØ§Ù…Ù„: Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù…Ø®ØªØµØ©ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø±
7. ÙˆØ¶Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø´Ø±ÙˆØ· ÙˆØ§Ù„Ø¶ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª
8. Ù‚Ø¯Ù… Ø³ÙŠØ± Ø¹Ù…Ù„ Ø¥Ø¬Ø±Ø§Ø¦ÙŠ ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù„Ù„Ù†Ù‡Ø§ÙŠØ©
9. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
10. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
11. Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ ÙˆØ§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„
12. Ø§Ø®ØªØªÙ… Ø¨Ù€: "ØªÙ†Ø¨ÙŠÙ‡: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·. Ø§Ø³ØªØ´Ø± Ù…Ø­Ø§Ù…ÙŠØ§Ù‹ Ù…Ø¤Ù‡Ù„Ø§Ù‹ Ù„Ù„Ù…Ø´ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø´Ø®ØµÙŠØ©."

Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:"""
            
            else:  # general, requirements, registration
                prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø©.

Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ©:
{context}{conversation_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­Ø§Ø³Ù…Ø© Ù„Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ (Ù‚Ø¯Ù… Ø£Ø´Ù…Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù…ÙƒÙ†Ø© Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„):
1. Ø§Ø¨Ø¯Ø£ Ø¨Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ù…Ù„ ÙŠÙ„Ø®Øµ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…ØªØ¨ÙˆØ¹Ø§Ù‹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ†
2. Ù†Ø¸Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ØµÙŠØºØ© Ù†Ù‚Ø§Ø· Ù…Ø±Ù‚Ù…Ø© Ù…ÙØµÙ„Ø© Ø¬Ø¯Ø§Ù‹ ÙˆØ´Ø§Ù…Ù„Ø©ØŒ ÙƒÙ„ Ù†Ù‚Ø·Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
   - Ø¹Ù†ÙˆØ§Ù† ÙˆØµÙÙŠ Ù…ÙØµÙ„ Ù…ØªØ¨ÙˆØ¹ Ø¨Ù†Ù‚Ø·ØªÙŠÙ† (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚)
   - Ù†Ù‚Ø§Ø· ÙØ±Ø¹ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ…ÙØµÙ„Ø© ØªØ¨Ø¯Ø£ Ø¨Ø´Ø±Ø·Ø© (-) ØªØ´Ø±Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø¬ÙˆØ§Ù†Ø¨
   - Ù…Ø±Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø­Ø¯Ø¯ Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© ÙØ±Ø¹ÙŠØ© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„ØµØ­ÙŠØ­Ø©
   - ØªÙˆØ¶ÙŠØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ÙˆØ£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹: "Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: [Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ±Ù‚Ù…Ù‡ ÙˆÙ„Ø³Ù†ØªÙ‡]ØŒ Ø§Ù„Ù…Ø§Ø¯Ø© [Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©]"
4. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² ØªÙ†Ø³ÙŠÙ‚ Ù…Ø«Ù„ ** Ø£Ùˆ * Ø£Ùˆ ##
5. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡ Ø¨Ø§Ù„ØµÙŠØºØ©: "Ø§Ù‚ØªØ¨Ø§Ø³: [Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø±ÙÙŠ]"
6. ÙˆØ¶Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª ÙˆØ§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
7. Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ©
8. Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØ¥Ø±Ø´Ø§Ø¯Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
9. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
10. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙˆØ§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ° Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆÙ…Ø§Øª
11. Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ ÙˆØ§Ù„Ù…Ø¯Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„
12. Ù‚Ø¯Ù… Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
13. Ø§Ø®ØªØªÙ… Ø¨Ù€: "ØªÙ†Ø¨ÙŠÙ‡: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø¹Ø§Ù… ÙÙ‚Ø·. Ø§Ø³ØªØ´Ø± Ù…Ø­Ø§Ù…ÙŠØ§Ù‹ Ù…Ø¤Ù‡Ù„Ø§Ù‹ Ù„Ù„Ù…Ø´ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø´Ø®ØµÙŠØ©."

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
        
        # Generate answer using direct HTTP API
        import os
        import requests
        import json
        
        try:
            # Direct HTTP client to avoid Railway interference
            api_key = os.getenv('OPENAI_API_KEY')
            openai_base_url = 'https://api.openai.com/v1/chat/completions'
            
            headers = {
                'Authorization': f"Bearer {api_key}",
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'gpt-4o',
                'messages': [{"role": "user", "content": prompt}],
                'temperature': 0.1,
                'max_tokens': 4000
            }
            
            response = requests.post(
                openai_base_url,
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                confidence = 0.85  # High confidence with advanced system
            else:
                print(f"âš ï¸ OpenAI API error: {response.status_code} - {response.text}")
                raise Exception(f"API Error: {response.status_code}")
            
        except Exception as e:
            if detected_language == 'english':
                answer = f"Sorry, an error occurred while processing the question: {str(e)}\n\nPlease try again or consult a specialized attorney."
            else:
                answer = f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}\n\nÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ùˆ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…Ø­Ø§Ù… Ù…Ø®ØªØµ."
            confidence = 0.0
        
        return QueryResult(
            answer=self.clean_response_formatting(answer),
            sources=sources,
            confidence=confidence,
            reasoning_steps=reasoning_steps,
            citations=citations,
            query_type=answer_type,
            processing_time=0.0  # Will be set by calling function
        )

class AdvancedLegalRAGSystem:
    """Main orchestrator for the advanced legal RAG system"""
    
    def __init__(self, documents_path: str = "mit_jordan_data/txt_output"):
        self.documents_path = Path(documents_path)
        self.processor = AdvancedLegalProcessor()
        self.vector_store = AdvancedVectorStore()
        self.reasoning_engine = LegalReasoningEngine(self.vector_store)
        self.documents: List[LegalDocument] = []
        
        print("ğŸš€ Advanced Legal RAG System initialized")
    
    def load_documents(self, force_rebuild: bool = False) -> int:
        """Load documents using pre-built embeddings"""
        print(f"ğŸ“ Loading documents using pre-built embeddings...")
        
        # Load pre-built embeddings directly
        if self.vector_store.load_prebuilt_embeddings():
            # Create placeholder documents for compatibility with existing code
            # The actual documents are already loaded in the vector store
            collection_count = self.vector_store.collection.count()
            print(f"ğŸ“š Successfully loaded {collection_count} document chunks from pre-built embeddings")
            
            # Create a minimal document list for compatibility
            self.documents = [LegalDocument(
                id="prebuilt_docs",
                title="Pre-built Legal Documents",
                source_file="embeddings_data.json",
                content="Loaded from pre-built embeddings",
                doc_type="Ù…ØªÙ†ÙˆØ¹Ø©",
                category="Ø¹Ø§Ù…",
                law_number=None,
                year=None,
                articles=[],
                metadata={"loaded_from": "pre-built embeddings"},
                word_count=0
            )]
            
            return collection_count
        else:
            print(f"âŒ Could not load pre-built embeddings")
            return 0
    
    def query(self, query: str, conversation_history: List[str] = None) -> QueryResult:
        """Process a legal query and return comprehensive result"""
        if not self.documents:
            return QueryResult(
                answer="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø¹Ø¯. ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£ÙˆÙ„Ø§Ù‹.",
                sources=[],
                confidence=0.0,
                reasoning_steps=[],
                citations=[],
                query_type="error",
                processing_time=0.0
            )
        
        print(f"ğŸ” Processing query: {query}")
        return self.reasoning_engine.process_query(query, conversation_history)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        if not self.documents:
            return {"status": "No documents loaded"}
        
        doc_types = {}
        categories = {}
        total_articles = 0
        
        for doc in self.documents:
            # Count document types
            doc_type = doc.doc_type
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            
            # Count categories
            category = doc.category
            categories[category] = categories.get(category, 0) + 1
            
            # Count articles
            total_articles += len(doc.articles)
        
        return {
            "total_documents": len(self.documents),
            "total_articles": total_articles,
            "document_types": doc_types,
            "legal_categories": categories,
            "vector_store_status": "Ready" if self.vector_store.tfidf_matrix is not None else "Not Ready",
            "average_articles_per_doc": total_articles / len(self.documents) if self.documents else 0
        }

# Main execution
if __name__ == "__main__":
    # Initialize the advanced system
    system = AdvancedLegalRAGSystem()
    
    # Load all documents
    num_docs = system.load_documents()
    
    if num_docs > 0:
        print(f"\nğŸ‰ Advanced Legal RAG System ready with {num_docs} documents!")
        print("\nğŸ“Š System Statistics:")
        stats = system.get_system_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        print("\nğŸš€ Ready to process legal queries!")
        print("Use: result = system.query('your question here')")
    else:
        print("âŒ No documents loaded. Please check the documents path.") 